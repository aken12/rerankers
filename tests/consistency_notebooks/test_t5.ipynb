{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = Qrels.from_ir_datasets(\"beir/scifact/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rerankers import Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "\n",
    "# corpus = [x for x in srsly.read_jsonl('data/wikipedia/data/collections/docs00.json')]\n",
    "queries = [x for x in srsly.read_json('data/msmarco/query/default_query.json')]\n",
    "\n",
    "# print(corpus[0])\n",
    "print(queries[0])\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = Reranker('castorini/monot5-base-msmarco-10k', device='cuda', batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = srsly.read_json('data/scifact/scifact_top_100.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_map = {x['_id']: f\"{x['title']} {x['text']}\" for x in corpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_dict = dict(qrels)\n",
    "queries = [q for q in queries if q['_id'] in qrels_dict]\n",
    "from tqdm import tqdm\n",
    "\n",
    "scores = {}\n",
    "for q in tqdm(queries):\n",
    "    doc_ids = top100[q['_id']]\n",
    "    docs = [corpus_map[x] for x in doc_ids]\n",
    "    scores[q['_id']] = ranker.rank(q['text'], docs, doc_ids=doc_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for q_id, ranked_results in scores.items():\n",
    "    top_10_results = ranked_results.top_k(10)\n",
    "    scores_dict[q_id] = {result.doc_id: result.score for result in top_10_results}\n",
    "run = Run(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import evaluate\n",
    "evaluation_score = evaluate(qrels, run, 'ndcg@10')\n",
    "litterature_result = 0.734 # From RankGPT Paper https://arxiv.org/pdf/2304.09542.pdf\n",
    "if abs(evaluation_score - litterature_result) > 0.01:\n",
    "    print(f\"Score {evaluation_score:0.3f} differs by more than 0.01 from the the reported score.\")\n",
    "else:\n",
    "    print(f\"Score is within 0.01 NDCG@10 of the reported score!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerankers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
